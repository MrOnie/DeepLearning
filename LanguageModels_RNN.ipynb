{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModels_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrOnie/DeepLearning/blob/master/LanguageModels_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8gZlVyCCbz",
        "colab_type": "text"
      },
      "source": [
        "# Laboratorio: Modelos del lenguaje con RNNs\n",
        "\n",
        "Resuelto por:\n",
        "- Diego Felipe Zabaleta Arias\n",
        "- Ronie Martinez Gordon\n",
        "\n",
        "En este laboratorio, vamos a entrenar un modelo del lenguaje basado en caracteres con Recurrent Neural Networks. Asimismo, utilizaremos el modelo para generar texto. En particular, alimentaremos nuestro modelo con obras de la literatura clásica en castellano para obtener una red neuronal que sea capaz de \"escribir\" fragmentos literarios.\n",
        "\n",
        "Los entrenamientos en este laboratorio para obtener un modelo de calidad podrían tomar cierto tiempo (5-10 minutos por epoch), por lo que se aconseja empezar a trabajar pronto. El uso de GPUs no ayuda tanto con LSTMs como con CNNs, por lo que si tenéis máquinas potentes en casa es posible que podáis entrenar más rápido o a la misma velocidad que en Colab. En todo caso, la potencia de Colab es más que suficiente para completar este laboratorio con éxito.\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d8/El_ingenioso_hidalgo_don_Quijote_de_la_Mancha.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
        "\n",
        "El dataset a utilizar consistirá en un archivo de texto con el contenido íntegro en castellano antiguo de El Ingenioso Hidalgo Don Quijote de la Mancha, disponible de manera libre en la página de [Project Gutenberg](https://www.gutenberg.org). Asimismo, como apartado optativo en este laboratorio se pueden utilizar otras fuentes de texto. Aquí podéis descargar los datos a utilizar de El Quijote y un par de obras adicionales:\n",
        "\n",
        "[El ingenioso hidalgo Don Quijote de la Mancha (Miguel de Cervantes)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io)\n",
        "\n",
        "[Compilación de obras teatrales (Calderón de la Barca)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219433&authkey=AKvGD6DC3IRBqmc)\n",
        "\n",
        "[Trafalgar (Benito Pérez Galdós)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ)\n",
        "\n",
        "Como ya deberíamos de estar acostumbrados en problemas de Machine Learning, es importante echar un vistazo a los datos antes de empezar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI274F8LQC59",
        "colab_type": "text"
      },
      "source": [
        "## 1. Carga y procesado del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZNnzvXuqVVm",
        "colab_type": "text"
      },
      "source": [
        "Primero, vamos a descargar el libro e inspeccionar los datos. El fichero a descargar es una versión en .txt del libro de Don Quijote, a la cual se le han borrado introducciones, licencias y otras secciones para dejarlo con el contenido real de la novela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7tKOZ9BFfki",
        "colab_type": "code",
        "outputId": "b383021a-9d13-4617-e5c5-a0992210801b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "import numpy as np \n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "import random\n",
        "import io\n",
        "%tensorflow_version 2.x magic:\n",
        "\n",
        "path = keras.utils.get_file(\n",
        "    fname=\"don_quijote.txt\", \n",
        "    origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.x magic:`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "Downloading data from https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io\n",
            "2154496/2151176 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGLvjLXrUUd",
        "colab_type": "text"
      },
      "source": [
        "Una vez descargado, vamos a leer el contenido del fichero en una variable. Adicionalmente, convertiremos el contenido del texto a minúsculas para ponérselo un poco más fácil a nuestro modelo (de modo que todas las letras sean minúsculas y el modelo no necesite diferenciar entre minúsculas y mayúsculas).\n",
        "\n",
        "**1.1.** Leer todo el contenido del fichero en una única variable ***text*** y convertir el string a minúsculas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WB6FejrrTu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fichero = open(path, 'r', encoding=\"utf8\") # Abrir fichero. Lectura.\n",
        "fichero_str = fichero.read() # Lectura del fichero\n",
        "text = fichero_str.lower() # cambiar a minúscula"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkgGl8GWtUk8",
        "colab_type": "text"
      },
      "source": [
        "Podemos comprobar ahora que efectivamente nuestra variable contiene el resultado deseado, con el comienzo tan característico del Quijote."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMFhe3COFwSD",
        "colab_type": "code",
        "outputId": "c3d8b242-68e5-44bb-cae3-7a2bec5d18b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"Longitud del texto: {}\".format(len(text)))\n",
        "text2 = text[0:300] # Para pruebas\n",
        "print(text2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longitud del texto: 2071198\n",
            "capítulo primero. que trata de la condición y ejercicio del famoso hidalgo\n",
            "don quijote de la mancha\n",
            "\n",
            "\n",
            "en un lugar de la mancha, de cuyo nombre no quiero acordarme, no ha mucho\n",
            "tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua,\n",
            "rocín flaco y galgo corredor. una olla de algo más\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ7TUXWiyvOj",
        "colab_type": "text"
      },
      "source": [
        "## 2. Procesado de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x66_Vi_Gyxns",
        "colab_type": "text"
      },
      "source": [
        "Una de las grandes ventajas de trabajar con modelos que utilizan caracteres en vez de palabras es que no necesitamos tokenizar el texto (partirlo palabra a palabra). Nuestro modelo funcionará directamente con los caracteres en el texto, incluyendo espacios, saltos de línea, etc.\n",
        "\n",
        "Antes de hacer nada, necesitamos procesar el texto en entradas y salidas compatibles con nuestro modelo. Como sabemos, un modelo del lenguaje con RNNs acepta una serie de caracteres y predice el siguiente carácter en la secuencia.\n",
        "\n",
        "* \"*El ingenioso don Qui*\" -> predicción: **j**\n",
        "* \"*El ingenioso don Quij*\" -> predicción: **o**\n",
        "\n",
        "De modo que la entrada y la salida de nuestro modelo necesita ser algo parecido a este esquema. En este punto, podríamos usar dos formas de preparar los datos para nuestro modelo.\n",
        "\n",
        "1. **Secuencia a secuencia**. La entrada de nuestro modelo sería una secuencia y la salida sería esa secuencia trasladada un caracter a la derecha, de modo que en cada instante de tiempo la RNN tiene que predecir el carácter siguiente. Por ejemplo:\n",
        "\n",
        ">* *Input*:   El ingenioso don Quijot \n",
        ">* *Output*: l ingenioso don Quijote\n",
        "\n",
        "2. **Secuencia a carácter**. En este variante, pasaríamos una secuencia de caracteres por nuestra RNN y, al llegar al final de la secuencia, predeciríamos el siguiente carácter.\n",
        "\n",
        ">* *Input*:   El ingenioso don Quijot \n",
        ">* *Output*: e\n",
        "\n",
        "En este laboratorio, por simplicidad, vamos a utilizar la segunda variante.\n",
        "\n",
        "De este modo, a partir del texto, hemos de generar nuestro propio training data que consista en secuencias de caracteres con el siguiente carácter a predecir. Para estandarizar las cosas, utilizaremos secuencias de tamaño *SEQ_LENGTH* caracteres (un hiperparámetro que podemos elegir nosotros).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfJUIxW5m5C",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1. Obtención de los caracteres y mapas de caracteres\n",
        "\n",
        "Antes que nada, necesitamos saber qué caracteres aparecen en el texto, ya que tendremos que diferenciarlos mediante un índice de 0 a *num_chars* - 1 en el modelo. Obtener:\n",
        " \n",
        "\n",
        "1.   Número de caracteres únicos que aparecen en el texto.\n",
        "2.   Diccionario que asocia char a índice único entre 0 y *num_chars* - 1. Por ejemplo, {'a': 0, 'b': 1, ...}\n",
        "3.   Diccionario reverso de índices a caracteres: {0: 'a', 1: 'b', ...}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVUN3JsqdZmY",
        "colab_type": "code",
        "outputId": "9131f004-68bb-4c6c-de40-aa6a5028e2ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Espacios, saltos de línea, signos de puntuación, vocales con tilde\n",
        "caracteres = []\n",
        "diccionario = {}\n",
        "diccionario_rev = {}\n",
        "indice = 0\n",
        "\n",
        "for letra in text:\n",
        "  if letra in diccionario:\n",
        "    pass #diccionario[letra]=diccionario[letra]+1 #contador\n",
        "  else:\n",
        "      caracteres.append(letra)\n",
        "      diccionario[letra] = indice\n",
        "      diccionario_rev[indice] = letra\n",
        "      indice += 1\n",
        "\n",
        "# Tamaño\n",
        "print(\"Número de caracteres únicos: \", len(caracteres))\n",
        "\n",
        "# Diccionario\n",
        "print(\"Diccionario:\")\n",
        "print(diccionario)\n",
        "\n",
        "# Diccionario reverso\n",
        "print(\"Diccionario reverso:\")\n",
        "print(diccionario_rev)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de caracteres únicos:  61\n",
            "Diccionario:\n",
            "{'c': 0, 'a': 1, 'p': 2, 'í': 3, 't': 4, 'u': 5, 'l': 6, 'o': 7, ' ': 8, 'r': 9, 'i': 10, 'm': 11, 'e': 12, '.': 13, 'q': 14, 'd': 15, 'n': 16, 'ó': 17, 'y': 18, 'j': 19, 'f': 20, 's': 21, 'h': 22, 'g': 23, '\\n': 24, ',': 25, 'b': 26, 'v': 27, 'z': 28, 'á': 29, 'ú': 30, 'ñ': 31, ';': 32, 'x': 33, 'é': 34, ':': 35, '-': 36, 'ü': 37, '¿': 38, \"'\": 39, '?': 40, '¡': 41, '!': 42, '«': 43, '»': 44, '(': 45, ')': 46, '\"': 47, 'ï': 48, 'w': 49, ']': 50, 'à': 51, '1': 52, '0': 53, '7': 54, '6': 55, '5': 56, '2': 57, '3': 58, '4': 59, 'ù': 60}\n",
            "Diccionario reverso:\n",
            "{0: 'c', 1: 'a', 2: 'p', 3: 'í', 4: 't', 5: 'u', 6: 'l', 7: 'o', 8: ' ', 9: 'r', 10: 'i', 11: 'm', 12: 'e', 13: '.', 14: 'q', 15: 'd', 16: 'n', 17: 'ó', 18: 'y', 19: 'j', 20: 'f', 21: 's', 22: 'h', 23: 'g', 24: '\\n', 25: ',', 26: 'b', 27: 'v', 28: 'z', 29: 'á', 30: 'ú', 31: 'ñ', 32: ';', 33: 'x', 34: 'é', 35: ':', 36: '-', 37: 'ü', 38: '¿', 39: \"'\", 40: '?', 41: '¡', 42: '!', 43: '«', 44: '»', 45: '(', 46: ')', 47: '\"', 48: 'ï', 49: 'w', 50: ']', 51: 'à', 52: '1', 53: '0', 54: '7', 55: '6', 56: '5', 57: '2', 58: '3', 59: '4', 60: 'ù'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_B4AWo0ElwA",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2. Obtención de secuencias de entrada y carácter a predecir\n",
        "\n",
        "Ahora, vamos a obtener las secuencias de entrada en formato texto y los correspondientes caracteres a predecir. Para ello, recorrer el texto completo leído anteriormente, obteniendo una secuencia de SEQ_LENGTH caracteres y el siguiente caracter a predecir. Una vez hecho, desplazarse un carácter a la izquierda y hacer lo mismo para obtener una nueva secuencia y predicción. Guardar las secuencias en una variable ***sequences*** y los caracteres a predecir en una variable ***next_chars***.\n",
        "\n",
        "Por ejemplo, si el texto fuera \"Don Quijote\" y SEQ_LENGTH fuese 5, tendríamos\n",
        "\n",
        "* *sequences* = [\"Don Q\", \"on Qu\", \"n Qui\", \" Quij\", \"Quijo\", \"uijot\"]\n",
        "* *next_chars* = ['u', 'i', 'j', 'o', 't', 'e']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NslxhnnDK6uA",
        "colab_type": "code",
        "outputId": "50a28377-823e-4db4-a6e2-d061f0658776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Definimos el tamaño de las secuencias. Puedes dejar este valor por defecto.\n",
        "SEQ_LENGTH = 30\n",
        "\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "## TU CÓDIGO AQUÍ\n",
        "for i in range (len(text)-SEQ_LENGTH):\n",
        "  #print(text[i:i+SEQ_LENGTH],text[i+SEQ_LENGTH])\n",
        "  sequences.append(text[i:i+SEQ_LENGTH])\n",
        "  next_chars.append(text[i+SEQ_LENGTH])\n",
        "\n",
        "print(sequences[0:10])\n",
        "print(next_chars[0:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['capítulo primero. que trata de', 'apítulo primero. que trata de ', 'pítulo primero. que trata de l', 'ítulo primero. que trata de la', 'tulo primero. que trata de la ', 'ulo primero. que trata de la c', 'lo primero. que trata de la co', 'o primero. que trata de la con', ' primero. que trata de la cond', 'primero. que trata de la condi']\n",
            "[' ', 'l', 'a', ' ', 'c', 'o', 'n', 'd', 'i', 'c']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y3AmjYtHdLJ",
        "colab_type": "text"
      },
      "source": [
        "Indicar el tamaño del training set que acabamos de generar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVWqKxFcbwTu",
        "colab_type": "code",
        "outputId": "66672db1-74a0-48b9-84d2-5f93faf1c14a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Tamaño del training set: \",len(sequences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamaño del training set:  2071168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goGQkKcwpLRJ",
        "colab_type": "text"
      },
      "source": [
        "Como el Quijote es muy largo y tenemos muchas secuencias, podríamos encontrar problemas de memoria. Por ello, vamos a elegir un número máximo de ellas. Si estás corriendo esto localmente y tienes problemas de memoria, puedes reducir el tamaño aún más, pero ten cuidado porque, a menos datos, peor calidad del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pm1Q19ppw8F",
        "colab_type": "code",
        "outputId": "3ff2c072-9a70-49f2-8250-d2253be00c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MAX_SEQUENCES = 500000 #disminuir si hay problemas de memoria a <len(sequences)\n",
        "\n",
        "perm = np.random.permutation(len(sequences))\n",
        "sequences, next_chars = np.array(sequences), np.array(next_chars)\n",
        "sequences, next_chars = sequences[perm], next_chars[perm]\n",
        "sequences, next_chars = list(sequences[:MAX_SEQUENCES]), list(next_chars[:MAX_SEQUENCES])\n",
        "\n",
        "print(len(sequences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FzgtAbPIs6f",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3. Obtención de input X y output y para el modelo\n",
        "\n",
        "Finalmente, a partir de los datos de entrenamiento que hemos generado vamos a crear los arrays de datos X e y que pasaremos a nuestro modelo.\n",
        "\n",
        "Para ello, vamos a utilizar *one-hot encoding* para nuestros caracteres. Por ejemplo, si sólo tuviéramos 4 caracteres (a, b, c, d), las representaciones serían: (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0) y (0, 0, 0, 1).\n",
        "\n",
        "De este modo, **X** tendrá shape *(num_sequences, seq_length, num_chars)* e **y** tendrá shape *(num_sequences, num_chars)*. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMBwZ9obNGNg",
        "colab_type": "code",
        "outputId": "2d49b4ff-1bff-46fb-a7a5-99e1f5992ab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.utils import np_utils\n",
        "NUM_CHARS = len(caracteres)  # Tu número de caracteres distintos aquí\n",
        "NUM_SEQUENCES = len(sequences)\n",
        "X = np.zeros((NUM_SEQUENCES, SEQ_LENGTH, NUM_CHARS))\n",
        "y = np.zeros((NUM_SEQUENCES, NUM_CHARS))\n",
        "\n",
        "## Tu código para rellenar X e y aquí. Pista: utilizar el diccionario de\n",
        "## chars a índices obtenido anteriormente junto con numpy. Por ejemplo,\n",
        "## si hacemos \n",
        "##     X[0, 1, char_to_indices['a']] = 1\n",
        "## estamos diciendo que para la segunda posición de la primera secuencia se\n",
        "## tiene una 'a'\n",
        "\n",
        "## TU CÓDIGO AQUÍ\n",
        "print (X.shape)\n",
        "print (y.shape)\n",
        "\n",
        "integer_encoded = [diccionario[char] for char in caracteres]\n",
        "\n",
        "onehot_encoded = list()\n",
        "for value in integer_encoded:\n",
        "    letter = [0 for _ in range(len(caracteres))]\n",
        "    letter[value] = 1\n",
        "    onehot_encoded.append(letter)\n",
        "\n",
        "for i in range(len(X)):\n",
        "  for j in range(len(X[i])):\n",
        "    X[i][j]=onehot_encoded[diccionario[sequences[i][j]]]\n",
        "\n",
        "for i in range(len(y)):\n",
        "  y[i]=onehot_encoded[diccionario[next_chars[i]]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500000, 30, 61)\n",
            "(500000, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeUxz3HPm3l",
        "colab_type": "text"
      },
      "source": [
        "## 3. Definición del modelo y entrenamiento\n",
        "\n",
        "Una vez tenemos ya todo preparado, es hora de definir el modelo. Define un modelo que utilice una **LSTM** con **128 unidades internas**. Si bien el modelo puede definirse de una manera más compleja, para empezar debería bastar con una LSTM más una capa Dense con el *softmax* que predice el siguiente caracter a producir. Adam puede ser una buena elección de optimizador.\n",
        "\n",
        "Una vez el modelo esté definido, entrénalo un poco para asegurarte de que la loss es decreciente. No es necesario guardar la salida de este entrenamiento en el entregable final, ya que vamos a hacer el entrenamiento más informativo en el siguiente punto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSw2j0btYWZs",
        "colab_type": "code",
        "outputId": "ff87b863-7de2-4155-c970-42f501b43e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "## Modelo\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]))) #128 unidades internas\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#model.fit(X, y, epochs=10, batch_size=128)\n",
        "#Se detiene el entrenamiento, luego de confirmar que la loss decrece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 128)               97280     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 61)                7869      \n",
            "=================================================================\n",
            "Total params: 105,149\n",
            "Trainable params: 105,149\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yUFHS4kHkyY",
        "colab_type": "text"
      },
      "source": [
        "Para ver cómo evoluciona nuestro modelo del lenguaje, vamos a generar texto según va entrenando. Para ello, vamos a programar una función que, utilizando el modelo en su estado actual, genere texto, con la idea de ver cómo se va generando texto al entrenar cada epoch.\n",
        "\n",
        "En el código de abajo podemos ver una función auxiliar para obtener valores de una distribución multinomial. Esta función se usará para muestrear el siguiente carácter a utilizar según las probabilidades de la salida de softmax (en vez de tomar directamente el valor con la máxima probabilidad, obtenemos un valor aleatorio según la distribución de probabilidad dada por softmax, de modo que nuestros resultados serán más diversos, pero seguirán teniendo \"sentido\" ya que el modelo tenderá a seleccionar valores con más probabilidad).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoGYpWOHd7Lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(probs, temperature=1.0):\n",
        "    \"\"\"Nos da el índice del elemento a elegir según la distribución\n",
        "    de probabilidad dada por probs.\n",
        "    \n",
        "    Args:\n",
        "      probs es la salida dada por una capa softmax:\n",
        "        probs = model.predict(x_to_predict)[0]\n",
        "      \n",
        "      temperature es un parámetro que nos permite obtener mayor\n",
        "        \"diversidad\" a la hora de obtener resultados. \n",
        "        \n",
        "        temperature = 1 nos da la distribución normal de softmax\n",
        "        0 < temperature < 1 hace que el sampling sea más conservador,\n",
        "          de modo que sampleamos cosas de las que estamos más seguros\n",
        "        temperature > 1 hace que los samplings sean más atrevidos,\n",
        "          eligiendo en más ocasiones clases con baja probabilidad.\n",
        "          Con esto, tenemos mayor diversidad pero se cometen más\n",
        "          errores.\n",
        "    \"\"\"\n",
        "    # Cast a float64 por motivos numéricos\n",
        "    probs = np.asarray(probs).astype('float64')\n",
        "    \n",
        "    # Hacemos logaritmo de probabilidades y aplicamos reducción\n",
        "    # por temperatura.\n",
        "    probs = np.log(probs) / temperature\n",
        "    \n",
        "    # Volvemos a aplicar exponencial y normalizamos de nuevo\n",
        "    exp_probs = np.exp(probs)\n",
        "    probs = exp_probs / np.sum(exp_probs)\n",
        "    \n",
        "    # Hacemos el sampling dadas las nuevas probabilidades\n",
        "    # de salida (ver doc. de np.random.multinomial)\n",
        "    samples = np.random.multinomial(1, probs, 1)\n",
        "    return np.argmax(samples)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fejfZldd4ou",
        "colab_type": "text"
      },
      "source": [
        "Utilizando la función anterior y el modelo entrenado, vamos a añadir un callback a nuestro modelo para que, según vaya entrenando, veamos los valores que resultan de generar textos con distintas temperaturas al acabar cada epoch.\n",
        "\n",
        "Para ello, abajo tenéis disponible el callback *on_epoch_end*. Esta función elige una secuencia de texto al azar en el texto disponible en la variable\n",
        "text y genera textos de longitud *GENERATED_TEXT_LENGTH* según las temperaturas en *TEMPERATURES_TO_TRY*, utilizando para ello la función *generate_text*.\n",
        "\n",
        "Completa la función *generate_text* de modo que utilicemos el modelo y la función sample para generar texto.\n",
        "\n",
        "NOTA: Cuando hagas model.predict, es aconsejable usar verbose=0 como argumento para evitar que la función imprima valores de salida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOEZvnBXkODd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEMPERATURES_TO_TRY = [0.2, 0.5, 1.0, 1.2]\n",
        "GENERATED_TEXT_LENGTH = 300\n",
        "\n",
        "def generate_text(seed_text, model, length, temperature=1):\n",
        "    \"\"\"Genera una secuencia de texto a partir de seed_text utilizando model.\n",
        "    \n",
        "    La secuencia tiene longitud length y el sampling se hace con la temperature\n",
        "    definida.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Aquí guardaremos nuestro texto generado, que incluirá el\n",
        "    # texto origen\n",
        "    generated = seed_text\n",
        "    \n",
        "    # Utilizar el modelo en un bucle de manera que generemos\n",
        "    # carácter a carácter. Habrá que construir los valores de\n",
        "    # X_pred de manera similar a como hemos hecho arriba, salvo que\n",
        "    # aquí sólo se necesita una oración\n",
        "    # Nótese que el x que utilicemos tiene que irse actualizando con\n",
        "    # los caracteres que se van generando. La secuencia de entrada al\n",
        "    # modelo tiene que ser una secuencia de tamaño SEQ_LENGTH que\n",
        "    # incluya el último caracter predicho.\n",
        " \n",
        "    for i in range(length):\n",
        "        xp = np.zeros((1, SEQ_LENGTH, len(caracteres)))       \n",
        "        for j in range(len(xp[0])):\n",
        "            xp[0][j]=onehot_encoded[diccionario[generated[j]]]        \n",
        "        \n",
        "        prediction = model.predict(xp, verbose=0)[0]\n",
        "        index = sample(prediction, temperature)\n",
        "        result = diccionario_rev[index]\n",
        "        generated = generated + result       \n",
        "        generated = generated[1:len(generated)]\n",
        "        \n",
        "        seed_text = seed_text + result\n",
        "    return seed_text\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "  print(\"\\n\\n\\n\")\n",
        "  \n",
        "  # Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "  # a partir de ella\n",
        "  start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "  for temperature in TEMPERATURES_TO_TRY:\n",
        "    print(\"------> Epoch: {} - Generando texto con temperature {}\".format(\n",
        "        epoch + 1, temperature))\n",
        "    \n",
        "    generated_text = generate_text(seed_text, model, \n",
        "                                   GENERATED_TEXT_LENGTH, temperature)\n",
        "    print(\"Seed: {}\".format(seed_text))\n",
        "    print(\"Texto generado: {}\".format(generated_text))\n",
        "    print()\n",
        "\n",
        "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSMYZ2JdrSJg",
        "colab_type": "text"
      },
      "source": [
        "Entrena ahora tu modelo. No te olvides de añadir *generation_callback* a la lista de callbacks utilizados en fit(). Ya que las métricas de clasificación no son tan críticas aquí (no nos importa tanto acertar el carácter exacto, sino obtener una distribución de probabilidad adecuada), no es necesario monitorizar la accuracy ni usar validation data, si bien puedes añadirlos para asegurarte de que todo está en orden.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oT7pNvjrP2e",
        "colab_type": "code",
        "outputId": "f956242d-3e93-4bf1-c8ab-20acfa414cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Entregable, generación con texto en cada epoch con diferentes temperaturas\n",
        "model.fit(X, y, epochs=10, batch_size=128, callbacks=[generation_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "500000/500000 [==============================] - 180s 361us/step - loss: 2.1895 - acc: 0.3449\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.2\n",
            "Seed: , y, mirando si acaso estaba a\n",
            "Texto generado: , y, mirando si acaso estaba a la mermera que se la me de la mería a la minte en la mería en con el caballero en que la mino de la mería me para de la mercienta de su la meras de la mencho en esto de su la con el con esta mería se con el conte de su mante de la merer estro en la minte de se para de la me me esto me haber al cond\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.5\n",
            "Seed: , y, mirando si acaso estaba a\n",
            "Texto generado: , y, mirando si acaso estaba al carallo que lo con mes cablesa el parla de la cuente, que los mestos que son el mabía me han hangon el codo la migar de que se la sencho, y con esto que secies y que su no mesto mis dos que san me vespordo, y en asergo de perde a rejures, y esto es estince mal sina de la mina de al conte harre ha \n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 1.0\n",
            "Seed: , y, mirando si acaso estaba a\n",
            "Texto generado: , y, mirando si acaso estaba asquiose tre qué los arame de no el hesecha do se uesengún esta con mondocier, y vertes, se sergunce sarme don pon estiuise.\n",
            "\n",
            "danto, nque empor, n egamo, dinin\n",
            "cuanto el se esto dellriena cante al romunto el ombar a munía me la\n",
            "ase que el se cabzadrasima sosodo los már amas por biebra no -de\n",
            "aa\n",
            "lado.\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 1.2\n",
            "Seed: , y, mirando si acaso estaba a\n",
            "Texto generado: , y, mirando si acaso estaba a sábía llejar, opió de cozimióno la proleme- los pose?\n",
            "\n",
            "mu\n",
            "sto, quijote tesor\n",
            "engo dimad denyo gucia llsos pianto., apqueloó y ertesnar bue ra!tí que eús ósquijóguras elrima condiente efaracorque\n",
            "omdi on la hlgra, tuy de las\n",
            "misí:\n",
            "ande eciodo la cérinra,\n",
            "a tudece pomorque lo recado la cahos sayáosta\n",
            "\n",
            "Epoch 2/10\n",
            "500000/500000 [==============================] - 175s 351us/step - loss: 1.8788 - acc: 0.4213\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.2\n",
            "Seed: a, que yo tengo escrita una\n",
            "ca\n",
            "Texto generado: a, que yo tengo escrita una\n",
            "caballero de la merado de la mala de la mercer con esto de la marado de los porque se los más que se la merces de la mancho de la caballero de la merces, y al cara de la merado de la mancho de la mus desto de por esto estan estan alegrado en esta merera de la meriero de la destar esta de la merme de l\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.5\n",
            "Seed: a, que yo tengo escrita una\n",
            "ca\n",
            "Texto generado: a, que yo tengo escrita una\n",
            "cantilos no desta prereros camo despodró en dujo el contila de la derceria el cual en cuere a su canta de los merías su sus camoses que se la había de se vestiero que antro por esto mus astos dijo que lo que podo su ton el cadarle, y su quieres los de por espor un malía me había la su haciento, a lo v\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 1.0\n",
            "Seed: a, que yo tengo escrita una\n",
            "ca\n",
            "Texto generado: a, que yo tengo escrita una\n",
            "carton compantoro, sancho se pos aal roztá hiceter,\n",
            "en pornatado estangomadas cobasdó una quernar ciaa y a la huciesto\n",
            "porque yo, hasví mijo-:\n",
            "\n",
            "muras había voste.\n",
            "\n",
            "-dieses bresaspejos, cospersecedo:\n",
            "\n",
            "-zon esta dindo dechieres bay asgigrases mue; senitado con sulos\n",
            "dica se les por los sos por el parmel\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 1.2\n",
            "Seed: a, que yo tengo escrita una\n",
            "ca\n",
            "Texto generado: a, que yo tengo escrita una\n",
            "camas pareñorquestro; poidías\n",
            "los encetiso\n",
            "suquiefio este al ansólete, a la hecmosor, a gune este ciltad si mejupa dos en don la sis\n",
            "lapichores uncenzoros suesto más cuentos blanciento, cieen, acuen, mevanzase, y hayerén parás, a mí sello, que ndifió de lbitar vuspero donrajo:\n",
            "\n",
            "-ano\n",
            "babena aszo no fac\n",
            "\n",
            "Epoch 3/10\n",
            "500000/500000 [==============================] - 174s 348us/step - loss: 1.7685 - acc: 0.4537\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.2\n",
            "Seed: o, y luego, de un brinco, la p\n",
            "Texto generado: o, y luego, de un brinco, la prestado a la caballero a mun puestra merced de la mano a la destan de su caballero de la mancho a la caballero a la caballero al cuerto a la para de la caballero que en la mando que no que ventado a la ventado a la caballero de la caballero al caballero a la caballero de la verdad de la contente de \n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.5\n",
            "Seed: o, y luego, de un brinco, la p\n",
            "Texto generado: o, y luego, de un brinco, la pretente con un me contentio de los hichos, con viento a lo que no de antaja se pera que aracer con el dijo en la allas los sundosos y comosado a él me encintado de su había de nuestra mermado por vuestra perdada antendo de lugar es los brandas allas de la había del caballero a su cabien a la de la d\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 1.0\n",
            "Seed: o, y luego, de un brinco, la p\n",
            "Texto generado: o, y luego, de un brinco, la percuelas mosermes acman sin tar tise a galerendá adan que se llo que halle tuda que enotentés. ¿prosicedejudo y oscastos yas\n",
            "catrisende las faedes dintu licabe que haramba las el mirarretil con caballora, y\n",
            "quiere las amondes, per en de vasiene.\n",
            "\n",
            "-yo hal sfe inazo:\n",
            "suncántadojas a, que in yagas se e\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 1.2\n",
            "Seed: o, y luego, de un brinco, la p\n",
            "Texto generado: o, y luego, de un brinco, la paredos. la musaredebonda dondé sobalo le\n",
            "tiefando\n",
            "visturte es puerme, nuegmansifilos más a vinavedotó de zune incimosió a comas con cuma de hayfaban:\n",
            "\n",
            "»puesmimunamasis mivados tobole de puesen rombis, buen ainncibalicudos? nuechosa que\n",
            "pinensuestro verto?, si más jaladove comivo suz\n",
            "que, con quién f\n",
            "\n",
            "Epoch 4/10\n",
            "500000/500000 [==============================] - 172s 345us/step - loss: 1.6930 - acc: 0.4769\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.2\n",
            "Seed: ormenten los malignos espíritu\n",
            "Texto generado: ormenten los malignos espírituras y al caballero al mancho a la caballero a la caballero a la caballero y de la merced la mano de la caballero al para que se la caballero de la merced de la proser de la pristar a la merced la merced la parecer de la despondió el caballero las caballeros y más de los descondió que el caballero de\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.5\n",
            "Seed: ormenten los malignos espíritu\n",
            "Texto generado: ormenten los malignos espírituras y astando las dejor y algún sancho, y su como se le\n",
            "aposado a por que el\n",
            "más que destad la hay razone los sengoso al cuanda en la que la comparte que está le denchado del conar a la repallar y de las descrotastas de las sis pondo del sermado a la ventar las castas del pir en la mancho el caballe\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 1.0\n",
            "Seed: ormenten los malignos espíritu\n",
            "Texto generado: ormenten los malignos espírituras del cabre yo algamio y trambre ponse hastro, ventió la faer en mazape, que idimle parsa a no se úto a el bueré a día meno.\n",
            "\n",
            "-y, como sépues escos su detio y atro por coclica;\n",
            "los tobar no\n",
            "se\n",
            "baí, andí o y\n",
            "a-dos algíno tal omiderque se algaria denos esperiase\n",
            "regunto de el mobarecio dellos\n",
            "derima\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 1.2\n",
            "Seed: ormenten los malignos espíritu\n",
            "Texto generado: ormenten los malignos espíritusas paja que habejidas del que ya\n",
            "he\n",
            "vise que enhodadura con el níhigo yo fueríar samogó al eévitño.\n",
            "\n",
            "-mí¿si en que gomo: ¿y crion se\n",
            "éstán entáberá beis y\n",
            "ramo, o que la mbembra que todocetera a rapundo tolica, que quedro, hiblezno a soy hegrod le, y el acíguido sancho que té\n",
            "ajensasa fistro,\n",
            "seunq\n",
            "\n",
            "Epoch 5/10\n",
            "500000/500000 [==============================] - 172s 344us/step - loss: 1.6343 - acc: 0.4944\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.2\n",
            "Seed: , la suple el agradecimiento. \n",
            "Texto generado: , la suple el agradecimiento. y esto de los de la mano de la ventida de la menciento de la mano de la caballero de los cualles que tenía en la caballero de la carte de la mencha de la caballero de la mano de la mano de los caballeros de las caballeros de la caballero alla de la merced que en la caballero al parte de la merced de\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.5\n",
            "Seed: , la suple el agradecimiento. \n",
            "Texto generado: , la suple el agradecimiento. con los cartas compesadar no pedría se le tal mancho que me la mera para su caballero de su casa, sancho del caballero alguna en la tanto, en la mano en la un carra, que me dijo lo que de algunos todos del moribieron de la fueron a todos hoces los enculicas, en tanta del cuerto le llegar no venía de\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 1.0\n",
            "Seed: , la suple el agradecimiento. \n",
            "Texto generado: , la suple el agradecimiento. ¿con hazó la caradura. pidió es muy que puedo sericiolé, quién ervilo y sí me pecuento, ni\n",
            "aendenanza en\n",
            "mi gustida, y queránque, quiéradón, y que aquí hoy\n",
            "ballera reuma. en lugo fuere recina y táy graza amano buen el dembresimeranta la cuallo de mi\n",
            "mendar que le mllo por que dios avesto venga conto\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 1.2\n",
            "Seed: , la suple el agradecimiento. \n",
            "Texto generado: , la suple el agradecimiento. que había poces vilas pomenguriad sermando\n",
            "competió en sambre, que me ósantra; moradón que quieren mosoror mis\n",
            "ordejas que tremos pantibo andierla hanrasen dela, controncio un volvideible a los cirdos dijo:\n",
            "\n",
            "  mquél, sip'e puntro el ciélo, y, en donque esto yo anchempló dál\n",
            "portia fuecal, ¿qué dorto\n",
            "\n",
            "Epoch 6/10\n",
            "500000/500000 [==============================] - 171s 342us/step - loss: 1.5898 - acc: 0.5077\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.2\n",
            "Seed: a, que alguna nueva\n",
            "locura le \n",
            "Texto generado: a, que alguna nueva\n",
            "locura le despondió a su descrito de la caballero alguna de la caballero de las caballeros de la mancho, y al caballero alguna de la caballero al caballero al caballero alguna de la caballero de la caballero de la caballero a su parecer de la caballero de la cual, y lo que el cual de la caballero de la conten\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.5\n",
            "Seed: a, que alguna nueva\n",
            "locura le \n",
            "Texto generado: a, que alguna nueva\n",
            "locura le había de la compada al caballero de dar a la mano, y no se halla de los acobasios y de\n",
            "hacer de mi bien a su desperdido en la ser todos algosos en las adondes.\n",
            "\n",
            "-pues con mi mancho -respondió sancho-, porque estaba compañaramente me quien puede a la cierta y la señora que no se la preguntado con la \n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 1.0\n",
            "Seed: a, que alguna nueva\n",
            "locura le \n",
            "Texto generado: a, que alguna nueva\n",
            "locura le pensadicur; y, señola, sino del\n",
            "capalle\n",
            "que vez tú sergonas,\n",
            "temos tiempo riparóla, otras, señor\n",
            "been adió el guitado padre que yo, acén ser de los resputillos,\n",
            " fenos, si frecieron el sigulia le fueranda angosudo las\n",
            "razoses, no lue zanesan; que buendo consigudares al caballo.\n",
            " con muvvidicedores t\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 1.2\n",
            "Seed: a, que alguna nueva\n",
            "locura le \n",
            "Texto generado: a, que alguna nueva\n",
            "locura le tajadoros vi tú len. vio, con\n",
            "las, con siendo llegatada pastapa, labre dulvinarado fuernen graza!\n",
            "\n",
            "mastó llgosibudres ollos!, que azosez el hueros más pustires que túlegoñale en guadabaza se mesos ootillante fi a contraba, genced cuén de\n",
            "habal descudero a traíne. -dejará seno: talmo de ir mumpañado \n",
            "\n",
            "Epoch 7/10\n",
            "500000/500000 [==============================] - 171s 341us/step - loss: 1.5555 - acc: 0.5171\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.2\n",
            "Seed: os que, si cae en ellos, le ha\n",
            "Texto generado: os que, si cae en ellos, le había de la ventura en la venta y la mano y en la señora, y a los amoris de los caballeros de la manera de la mancho, y al camino de la caballero de la mancho, y a la contento de la manto, y a la corancia y de los caballeros y al caballeros de los caballeros de la manera, y a la mano de la caballero d\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.5\n",
            "Seed: os que, si cae en ellos, le ha\n",
            "Texto generado: os que, si cae en ellos, le había en sus fintamos de caballería.\n",
            "\n",
            "-¿con lo que el sino -dijo sancho-, a quieres contentar su casa le había de la merced de cuerpo el mandado que se le dicen se sobre de la caballería, y las labras de las desmancias de la mario\n",
            "a la corazar de la esterma y hacer a su printo de los que en la visto d\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 1.0\n",
            "Seed: os que, si cae en ellos, le ha\n",
            "Texto generado: os que, si cae en ellos, le has caddia de tanta del dienos que, allí, que se neñor de camina\n",
            "continzertana.\n",
            "\n",
            "-eso -señoras; a buen este, sé tierr se llegón fin mí tan el apido dio más picho, cueño dallos intunes que de govella, y liera los nincutes malas con las suestras, terí, y de dulpenta, y por ducho que, con esta de rollos \n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 1.2\n",
            "Seed: os que, si cae en ellos, le ha\n",
            "Texto generado: os que, si cae en ellos, le haza:\n",
            "\n",
            "-¿quién tás nigo; en jumiso, me\n",
            "don\n",
            "quijote a crífocirados, terna impañad?. ajarmané por eso deoneta?.\n",
            "\n",
            "-cubos dé cósita -respostiame-jondes castiblos.\n",
            " santóles; aunque no más viene una conrecia, que llevaré soga ancuble, que el valo llegada, no quien luego de cura. el libréso los\n",
            "escartóseatr\n",
            "\n",
            "Epoch 8/10\n",
            "500000/500000 [==============================] - 169s 338us/step - loss: 1.5269 - acc: 0.5257\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.2\n",
            "Seed: imiento, y si no, no digas nad\n",
            "Texto generado: imiento, y si no, no digas nada esta merced de la caballero de los desentenios de la mundo de los caballeros de la para de la menor de la mano de la contenta de la mancho, y al cual ha de su caballero de los caballeros de la mano de la mano de la mano de la mano de la señora de la mano de la mano de la mano de la mano de la mano\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.5\n",
            "Seed: imiento, y si no, no digas nad\n",
            "Texto generado: imiento, y si no, no digas nada de la hiciento, dice que la vinido por la verdad de la vigar a las manos en la menar a dieron a las cosas que se le prover del caballero desposimiento de contigua, que los de los dientes le dijo a\n",
            "todo es de los algunas vinidos y de la menos, y a lo que no es mi mermar si en mi lo que se vengan de\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 1.0\n",
            "Seed: imiento, y si no, no digas nad\n",
            "Texto generado: imiento, y si no, no digas nadienes horroso empeño dabal un viendo algún ver por respondió de vencido y desganabamos; y, se acoraguilás, don quijote lo que penga\n",
            "solor de áis que el más, hallarmo cuento a la faltar apánilla que he tracieso milo las proseros traía, y que no era si, dergosa\n",
            "menorez lo que\n",
            "nuega llegaron a no si le\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 1.2\n",
            "Seed: imiento, y si no, no digas nad\n",
            "Texto generado: imiento, y si no, no digas nada orómitárole. porque su cresas a junerlo su toma que el mirle a pie tobo que\n",
            "haciéndoles: le vuestra resfingurado. en comida, gomerrí,\n",
            "dura que era sentibuel toda esbrina, dice los sol mercejas que le ancuén justarse que la caballero, sin pudienra; envías y emperamidos apares en tan tosuchaspo?\n",
            "-¡q\n",
            "\n",
            "Epoch 9/10\n",
            "500000/500000 [==============================] - 169s 338us/step - loss: 1.5039 - acc: 0.5317\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.2\n",
            "Seed: co reales, que, juntándose a\n",
            "l\n",
            "Texto generado: co reales, que, juntándose a\n",
            "la caballero ante alguna alguna con la mano de la muerte de la mano de los caballeros de la mano de la contar a la caballero de la mano y la contentado de la caballero de la caballero de la mano, y así es la ventura de la mano de la caballero de la mano de la mano de la caballero de la mano de la cab\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.5\n",
            "Seed: co reales, que, juntándose a\n",
            "l\n",
            "Texto generado: co reales, que, juntándose a\n",
            "la muya de la mano y malo en la mala don la caballero de las vertes le había despidica de la verdad, y al caballero su historia, y mal cual había alguno por la caballero antes de contrado con ella en la hermosa, y el\n",
            "valor de las cosas con caballeros. con todo sancho a su parecer de las oche por espa\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 1.0\n",
            "Seed: co reales, que, juntándose a\n",
            "l\n",
            "Texto generado: co reales, que, juntándose a\n",
            "la de caballero,\n",
            "   a especión ni encérvieres.\n",
            "tresemos persoso.\n",
            " llovía, quieres herredmenciado de dula las necesase\n",
            "otro, para puñó tan alguno. \n",
            "ciertos y entiences an tentita, prastiviéndoletes!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "de curjadára cuedros -cinerte-; y mi pedrecerde lo que me\n",
            "meterá mujes, así, que ella; que, paro po\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 1.2\n",
            "Seed: co reales, que, juntándose a\n",
            "l\n",
            "Texto generado: co reales, que, juntándose a\n",
            "los labíador su gustro, porto de\n",
            "a esperio y la\n",
            "sintilado, para yo princira, se tenta innigué de\n",
            "míjo; porque enceldó\n",
            "a stiénos: mur mucha ínsibigo. sempió un\n",
            "vies, no comientigo, golignos, con que vende, a lo amuderoimo en secho,\n",
            "aviado pustoras y este vana de todo\n",
            "el vintra yo su malo tenga\n",
            "hastesi\n",
            "\n",
            "Epoch 10/10\n",
            "500000/500000 [==============================] - 169s 337us/step - loss: 1.4844 - acc: 0.5376\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.2\n",
            "Seed: ad. y si esto es verdad, como \n",
            "Texto generado: ad. y si esto es verdad, como se le alguna por la menos de los de los que le puso en la caballero al parte de la cortesio de la mano de la caballero de la merced que en la cuerta y en la caballero de la mano de la muerte de la mancho panza y el cura que le parece y la caballero de los cuales se le parece de la caballero de los c\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.5\n",
            "Seed: ad. y si esto es verdad, como \n",
            "Texto generado: ad. y si esto es verdad, como ser los\n",
            "rocinantes de los estaban que tenía a los costos de la parte de mis señoras jaustas pastos de las\n",
            "dejeses se desponimente de la su desenciendo mucho plaza y espada y de las caballeros, y al cura le andan a la cabellero.\n",
            "\n",
            "-señor saca de lo que aportura en la merced que están otros mercedes de\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 1.0\n",
            "Seed: ad. y si esto es verdad, como \n",
            "Texto generado: ad. y si esto es verdad, como dijose todo la cuando sanda;\n",
            "\n",
            "\n",
            "  dartan aloditar tenta.\n",
            "\n",
            "y, acuáren seres y hallarmente.\n",
            "\n",
            "el amos la estendido\n",
            "esto de profesamente gentuda que les, como sintes entedos, sin quita\n",
            "ni por engantísime que en la españa de su ande escuerta por su regente.»\n",
            "\n",
            "posagan lo preseal; y, para que sendí tratos y\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 1.2\n",
            "Seed: ad. y si esto es verdad, como \n",
            "Texto generado: ad. y si esto es verdad, como incostó ser\n",
            "escuyendoños\n",
            "andismhas; bearse la vicieres;,\n",
            "hazán la\n",
            "pesalda, gosedaz de  mirce a quiso que luega\n",
            "apusimól! sincirno\n",
            "panzan infila,\n",
            "y no nubres por\n",
            "gallarnaro no álbróle que canta lugariéndale llo? y, por jueta que s\n",
            "quién taz estripetemenuen que lo un berma nodad; a dequí,\n",
            "deschuina, h\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f985f434f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9skJppUNBTYw",
        "colab_type": "text"
      },
      "source": [
        "Prueba: generación de texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_hpb3cJSddu",
        "colab_type": "code",
        "outputId": "cd519191-7fac-492a-da83-5d3476fcdd8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Generador para pruebas\n",
        "start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "generated_text = generate_text(seed_text, model, GENERATED_TEXT_LENGTH, 0.5)\n",
        "print(\"Texto generado: {}\".format(generated_text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Texto generado: go, no lleva pergenio de volver\n",
            "en la verdad que se me parte en las pasas que el cura de mi señor con el gobernada y de don quijote y se puede para que el cura de la menos estarlos el mande estos prestosis, y no había sentiman de poso, por como la prosegue a ver para le suy de los pensandos de la caballero tran mala de su parte \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBbmz9DMhVhc",
        "colab_type": "text"
      },
      "source": [
        "## Entregable\n",
        "\n",
        "Completa los apartados anteriores para entrenar modelos del lenguaje que sean capaces de generar texto con cierto sentido. Comentar los resultados obtenidos y cómo el modelo va mejorando época a época. Comentar las diferencias apreciadas al utilizar diferentes valores de temperatura. Entregar al menos la salida de un entrenamiento completo con los textos generados época a época.\n",
        "\n",
        "El objetivo no es conseguir generar pasajes literarios con coherencia, sino obtener lenguaje que se asemeje en cierta manera a lo visto en el texto original y donde las palabras sean reconocibles como construcciones en castellano. Como ejemplo de lo que se puede conseguir, este es el resultado de generar texto después de 10 epochs y con temperature 0.2:\n",
        "\n",
        "\n",
        "```\n",
        "-----> Epoch: 10 - Generando texto con temperature 0.2\n",
        "Seed: o le cautivaron y rindieron el\n",
        "Texto generado: o le cautivaron y rindieron el caballero de la caballería de la mano de la caballería del cual se le dijo:\n",
        "\n",
        "-¿quién es el verdad de la caballería de la caballería de la caballería de la caballería de la caballería, y me ha de habían de la mano que el caballero de la mano de la caballería. y que no se le habían de la mano de la c\n",
        "\n",
        "```\n",
        "\n",
        "Asimismo, se proponen los siguientes aspectos opcionales para conseguir nota extra:\n",
        "\n",
        "*   Experimentar con los textos de teatro en verso de Calderón de la Barca (¿es capaz el modelo de aprender las estructuras del teatro en verso?) o con alguno de los otros textos disponibles. También se puede probar con textos de vuestra elección.\n",
        "*   Experimentar con distintos valores de SEQ_LENGTH.\n",
        "*   Experimentar con los hiperparámetros del modelo o probar otro tipo de modelos como GRUs o *stacked* RNNs (RNNs apiladas).\n",
        "*   Experimentar utilizando embeddings en vez de representaciones one-hot.\n",
        "*   (Difícil) Entrenar un modelo secuencia a secuencia en vez de secuencia a carácter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAtdV72jSpU3",
        "colab_type": "text"
      },
      "source": [
        "## Conclusión\n",
        "A medida que el modelo avanza en su entrenamiento se nota que comete menos errores con las palabras, las frases no tienen sentido pero respeta la ortografia y estilos del escritor:\n",
        "\n",
        "```\n",
        "------> Epoch: 7 - Generando texto con temperature 0.5\n",
        "Seed: os que, si cae en ellos, le ha\n",
        "Texto generado: os que, si cae en ellos, le había en sus fintamos de caballería.\n",
        "\n",
        "-¿con lo que el sino -dijo sancho-, a quieres contentar su casa le había de la merced de cuerpo el mandado que se le dicen se sobre de la caballería, y las labras de las desmancias de la mario\n",
        "a la corazar de la esterma y hacer a su printo de los que en la visto d\n",
        "```\n",
        "\n",
        "Con respecto a la temperatura, a mayor valor se genera mas \"basura\", aparecen palabras que no existen y deja de obedecer orden gramatical. Tal como se comenta en la documentación de la función \"sample\": El valor de temperature = 1 nos da la distribución normal de softmax. Valores de temperature entre 0 y 1 hace que el sampling sea más conservador, mientras que para valores mayores a 1 hace que los samplings sean más atrevidos, es decir, elige con mayor frecuencia clases con baja probabilidad.\n",
        "\n",
        "Como curiosidad, muchos resultados de las epochs con temerature = 0.2 utilizan la palabra \"caballero\" con alta frecuencia.\n",
        "\n",
        "Utilizando Colab, cada epoch tarda en ejecutarse alrededor de 170 segundos, utilizando un train set de 500000 (el texto original es de tamaño 2071168). Además se utiliza en gran parte los recursos proporcionados por esta plataforma:\n",
        "![image.png](data:image/png;base64,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)"
      ]
    }
  ]
}